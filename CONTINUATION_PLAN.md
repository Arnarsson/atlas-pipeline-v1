# Atlas Data Pipeline - Continuation Plan
**Date**: January 11, 2026
**Current Status**: 84% Complete (Atlas Data Pipeline Standard)
**Recent Work**: 3 PRs with codecs collaboration (normalization, API fixes, dashboard stats)

---

## ğŸ¯ Phase 1: Integration & Consolidation (IMMEDIATE)

### **1.1 Merge Outstanding Work** â±ï¸ 30 minutes
**Priority**: CRITICAL

Current state:
- âœ… **PR/1** (`pr/1`): Data normalization - Ready to merge
- âœ… **PR/2** (`pr/2`): API contract fixes - Ready to merge
- âœ… **Dashboard Stats** (`feature/dashboard-stats`): Dashboard endpoint - **MUST MERGE**

**Actions**:
```bash
# Step 1: Merge dashboard-stats (has latest dashboard.py)
git checkout feature/api-contract-fix-and-tests
git merge feature/dashboard-stats

# Step 2: Cherry-pick normalization improvements from pr/1
git cherry-pick c6d5782  # Data normalization commit

# Step 3: Verify no conflicts with pr/2 work (already on current branch)
git log --oneline -5
```

**Expected Outcome**:
- Single unified branch with all improvements
- Dashboard stats endpoint functional
- Data normalization applied
- API contracts aligned

---

### **1.2 Comprehensive Testing** â±ï¸ 45 minutes
**Priority**: HIGH

**Backend Tests**:
```bash
cd backend
source venv/bin/activate
pytest tests/ -v
./scripts/verify-setup.sh
```

**Frontend E2E Tests**:
```bash
cd frontend
npm run test:e2e
```

**Integration Test**:
```bash
# 1. Start backend
python3 simple_main.py

# 2. Start frontend (new terminal)
cd frontend && npm run dev

# 3. Upload test CSV and verify:
#    - All 6 quality dimensions display
#    - PII detection works
#    - Dashboard stats load correctly
#    - No console errors
```

**Verification Checklist**:
- [ ] Backend: 82/82 tests passing
- [ ] Frontend: 122+ E2E tests passing
- [ ] Dashboard stats endpoint returns data
- [ ] CSV upload â†’ Quality + PII analysis works
- [ ] No console TypeError errors
- [ ] All 9 pages navigate correctly

---

### **1.3 Update Documentation** â±ï¸ 20 minutes
**Priority**: MEDIUM

**Update CLAUDE.md**:
- âœ… Mark dashboard stats as FIXED (no longer 404)
- âœ… Update test count (may have increased)
- âœ… Document new normalization features
- âœ… Update "Recent Updates" section with merge info

**Create MERGE_NOTES.md**:
```markdown
# Branch Merge Summary - Jan 11, 2026

## Merged Branches
1. feature/dashboard-stats (5e67503) - Dashboard stats endpoint
2. pr/1 data normalization (c6d5782) - Client normalization
3. pr/2 already integrated - API contract fixes

## New Features
- /dashboard/stats endpoint with 5 metrics
- Comprehensive data normalization layer
- Enhanced error handling with ErrorBoundary

## Testing Updates
- Added test_dashboard.py (backend)
- Enhanced E2E tests for new features
```

---

## ğŸš€ Phase 2: Fill Critical Gaps (16% â†’ 90%)

### **2.1 Advanced Connectors** â±ï¸ 8-12 hours
**Priority**: HIGH (adds 6% to completion)

**Target**: Add 2 high-value connectors

#### **Option A: Salesforce CRM Connector**
**Business Value**: HIGH (CRM data is critical for analytics)

```python
# backend/app/connectors/salesforce_connector.py
class SalesforceConnector:
    """Salesforce CRM connector with OAuth2."""

    def __init__(self, config):
        self.instance_url = config["instance_url"]
        self.access_token = config["access_token"]

    async def fetch_data(self, object_type: str):
        # Fetch Accounts, Contacts, Opportunities, etc.
        # Use Salesforce REST API
        # Handle pagination and rate limits
```

**Features**:
- OAuth2 authentication
- Fetch standard objects (Account, Contact, Opportunity, Lead)
- Custom object support
- Field mapping to Atlas schema
- Incremental sync based on `LastModifiedDate`

**Estimated**: 6 hours

---

#### **Option B: Google Sheets Connector**
**Business Value**: MEDIUM-HIGH (many businesses use Sheets as DB)

```python
# backend/app/connectors/google_sheets_connector.py
class GoogleSheetsConnector:
    """Google Sheets connector with service account auth."""

    def __init__(self, config):
        self.spreadsheet_id = config["spreadsheet_id"]
        self.credentials = config["service_account_json"]

    async def fetch_data(self, sheet_name: str):
        # Fetch sheet data via Google Sheets API
        # Auto-detect column types
        # Handle multiple sheets
```

**Features**:
- Service account or OAuth2 authentication
- Multi-sheet support
- Auto-detect headers and data types
- Real-time updates via webhooks (optional)
- Export capability (read + write)

**Estimated**: 4 hours

---

### **2.2 Real-time Streaming (Kafka)** â±ï¸ 12-16 hours
**Priority**: MEDIUM (adds 4% to completion)

**Goal**: Enable real-time data ingestion

```python
# backend/app/streaming/kafka_consumer.py
class KafkaStreamProcessor:
    """Process real-time data from Kafka topics."""

    async def consume_topic(self, topic: str):
        # Consume messages from Kafka
        # Apply PII detection in real-time
        # Stream to Explore layer
        # Trigger quality checks on batches
```

**Features**:
- Kafka topic subscription
- Real-time PII masking
- Micro-batch quality validation (every 100 records)
- Stream to Explore â†’ Chart â†’ Navigate
- Offset management and recovery

**Estimated**: 12 hours

---

### **2.3 Advanced RBAC** â±ï¸ 6-8 hours
**Priority**: MEDIUM (adds 3% to completion)

**Goal**: Multi-tenant access control

```python
# backend/app/auth/rbac.py
class RBACManager:
    """Role-based access control for datasets and features."""

    ROLES = {
        "admin": ["read", "write", "delete", "manage_users"],
        "data_engineer": ["read", "write", "run_pipelines"],
        "analyst": ["read", "export"],
        "viewer": ["read"]
    }
```

**Features**:
- JWT-based authentication
- Role hierarchy (Admin > Engineer > Analyst > Viewer)
- Dataset-level permissions
- Feature store access control
- Audit logging for all access

**Estimated**: 8 hours

---

### **2.4 ML Model Tracking** â±ï¸ 6-8 hours
**Priority**: MEDIUM (adds 3% to completion)

**Goal**: Track ML models trained on Atlas data

```python
# backend/app/ml/model_registry.py
class ModelRegistry:
    """Register and track ML models."""

    async def register_model(self, model_info: dict):
        # Register model with metadata
        # Link to feature group version
        # Track performance metrics
        # Store model artifacts (S3/GCS)
```

**Features**:
- Model versioning (semantic versioning)
- Link models to feature group versions
- Performance metrics tracking (accuracy, F1, etc.)
- A/B test tracking
- Model lineage (which data produced which model)

**Estimated**: 6 hours

---

## ğŸ“Š Phase 3: Production Hardening (90% â†’ 95%)

### **3.1 Monitoring & Observability** â±ï¸ 8-10 hours
**Priority**: HIGH

**Components**:
1. **Prometheus Metrics**
   - Pipeline execution time
   - Quality score trends
   - PII detection counts
   - API response times
   - Database connection pool stats

2. **Grafana Dashboards**
   - System health dashboard
   - Data quality trends
   - Pipeline performance
   - User activity
   - Cost tracking (if cloud-based)

3. **Alerting**
   - Quality score drops below threshold
   - Pipeline failures
   - High PII detection rates
   - API errors spike
   - Database connection issues

**Estimated**: 10 hours

---

### **3.2 CI/CD Pipeline** â±ï¸ 6-8 hours
**Priority**: MEDIUM-HIGH

**GitHub Actions Workflow**:
```yaml
# .github/workflows/atlas-ci.yml
name: Atlas Pipeline CI/CD

on: [push, pull_request]

jobs:
  test:
    - Backend tests (pytest)
    - Frontend tests (Playwright)
    - Integration tests

  build:
    - Docker image build
    - Push to registry

  deploy:
    - Deploy to staging (auto)
    - Deploy to production (manual approval)
```

**Features**:
- Automated testing on every PR
- Docker image building
- Automated deployment to staging
- Manual approval for production
- Rollback capability

**Estimated**: 8 hours

---

### **3.3 Backup & Disaster Recovery** â±ï¸ 4-6 hours
**Priority**: MEDIUM

**Components**:
1. **Database Backups**
   - Automated daily PostgreSQL dumps
   - Point-in-time recovery (WAL archiving)
   - Backup retention policy (30 days)

2. **Data Layer Backups**
   - Explore layer (raw data) - S3/GCS backup
   - Feature store versions - immutable storage
   - Configuration backups

3. **Recovery Procedures**
   - Documented recovery steps
   - Recovery time objective (RTO): < 4 hours
   - Recovery point objective (RPO): < 1 hour

**Estimated**: 6 hours

---

## ğŸ¯ Phase 4: Advanced Features (95% â†’ 100%)

### **4.1 Data Catalog Enhancements** â±ï¸ 4-6 hours
**Priority**: LOW-MEDIUM

**New Features**:
- **Smart Search**: Full-text search across schemas, descriptions, tags
- **Data Lineage Visualization**: Interactive graph (D3.js/Cytoscape)
- **Usage Analytics**: Track which datasets are most queried
- **Data Profiling**: Statistical summaries, histograms, distributions
- **Collaboration**: Comments, annotations, dataset ratings

**Estimated**: 6 hours

---

### **4.2 Advanced Quality Rules** â±ï¸ 6-8 hours
**Priority**: LOW-MEDIUM

**Custom Rules Engine**:
```python
# backend/app/pipeline/quality/custom_rules.py
class CustomQualityRule:
    """Define custom quality validation rules."""

    def __init__(self, rule_config: dict):
        self.name = rule_config["name"]
        self.condition = rule_config["condition"]
        self.threshold = rule_config["threshold"]

    async def validate(self, data: pd.DataFrame) -> bool:
        # Evaluate custom condition
        # Return pass/fail with details
```

**Features**:
- User-defined validation rules
- SQL-like condition syntax
- Cross-column validation
- Time-series anomaly detection
- Automated remediation suggestions

**Estimated**: 8 hours

---

### **4.3 API Gateway & Rate Limiting** â±ï¸ 4-6 hours
**Priority**: LOW

**Components**:
- API key management
- Rate limiting (per user/API key)
- Request throttling
- Usage quotas
- API analytics dashboard

**Estimated**: 6 hours

---

## ğŸ“‹ Recommended Execution Order

### **Sprint 1: Integration (Week 1)** â±ï¸ ~8 hours
1. âœ… Merge all branches (1.1)
2. âœ… Comprehensive testing (1.2)
3. âœ… Update documentation (1.3)
4. ğŸš€ Add Salesforce connector (2.1A)
5. ğŸš€ Add Google Sheets connector (2.1B)

**Outcome**: Clean codebase, 2 new connectors â†’ **90% complete**

---

### **Sprint 2: Production Ready (Week 2)** â±ï¸ ~12 hours
1. ğŸš€ Monitoring & observability (3.1)
2. ğŸš€ CI/CD pipeline (3.2)
3. ğŸš€ Backup & recovery (3.3)
4. ğŸš€ Advanced RBAC (2.3)

**Outcome**: Production-grade system â†’ **95% complete**

---

### **Sprint 3: Advanced (Week 3)** â±ï¸ ~16 hours
1. ğŸš€ Real-time streaming (2.2)
2. ğŸš€ ML model tracking (2.4)
3. ğŸš€ Data catalog enhancements (4.1)
4. ğŸš€ Custom quality rules (4.2)

**Outcome**: Enterprise-ready platform â†’ **100% complete**

---

## ğŸ¯ Alternative: Quick Wins Path (1 Week)

If time is limited, focus on **high-impact, low-effort** features:

### **Day 1-2: Merge & Test** â±ï¸ 4 hours
- Merge branches
- Test everything
- Update docs

### **Day 3-4: Top 2 Connectors** â±ï¸ 10 hours
- Google Sheets (easier, 4h)
- Salesforce (high value, 6h)

### **Day 5: Monitoring** â±ï¸ 6 hours
- Basic Prometheus metrics
- Simple Grafana dashboard
- Critical alerts

**Total**: 20 hours â†’ **92% complete** with high-value additions

---

## ğŸ“Š Completion Roadmap

| Phase | Features | Hours | Completion % |
|-------|----------|-------|--------------|
| **Current** | Core platform, 3 connectors, GDPR, Quality, PII | - | **84%** |
| **Phase 1** | Merge branches, testing | 2h | **84%** (consolidated) |
| **Phase 2** | +2 connectors, RBAC, ML tracking | 20h | **90%** |
| **Phase 3** | Monitoring, CI/CD, backups | 24h | **95%** |
| **Phase 4** | Streaming, advanced features | 30h | **100%** |

**Total Time to 100%**: ~76 hours (~2-3 weeks at 30h/week)

---

## ğŸ¯ Success Metrics

### **Technical Metrics**
- [ ] All tests passing (backend + frontend)
- [ ] API response time < 200ms (p95)
- [ ] Zero console errors in frontend
- [ ] Database queries optimized (< 100ms)
- [ ] PII detection accuracy > 99%

### **Feature Metrics**
- [ ] 5+ data connectors operational
- [ ] RBAC protecting all endpoints
- [ ] ML model tracking in use
- [ ] Real-time streaming operational
- [ ] Monitoring dashboards deployed

### **Production Metrics**
- [ ] CI/CD pipeline automated
- [ ] Backups running daily
- [ ] Alerts configured and tested
- [ ] Documentation complete
- [ ] Deployment runbooks created

---

## ğŸ“ Next Steps (RIGHT NOW)

```bash
# 1. Merge dashboard-stats branch
git merge feature/dashboard-stats

# 2. Test the system
cd backend && python3 simple_main.py
cd frontend && npm run dev

# 3. Verify dashboard stats endpoint
curl http://localhost:8000/dashboard/stats | jq

# 4. Choose next feature from Phase 2
#    Recommendation: Start with Google Sheets connector (easiest, 4h)
```

---

**Ready to continue! Which phase should we tackle first?** ğŸš€
